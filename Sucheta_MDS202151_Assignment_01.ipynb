{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ],
      "metadata": {
        "id": "n-wpogQU7Vst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgyO9cB1H6d_",
        "outputId": "d812d6dd-1b92-43fc-f600-0c692f60bfae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFUmswEnNaFP"
      },
      "outputs": [],
      "source": [
        "#import the required libraries for loading the file\n",
        "import json\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWzvEIcv558r"
      },
      "outputs": [],
      "source": [
        "#Extract all the files from zip folder\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('pdf_json.zip','r') as z:\n",
        "  z.extractall('/content/files/')\n",
        "z.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUgfEbyBx7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf403db-e18f-45f4-f19e-c72d576e97e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56529"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#load the file\n",
        "files = os.listdir('/content/files/pdf_json/')\n",
        "len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjvKJXK8P8uJ"
      },
      "outputs": [],
      "source": [
        "#function to extract text from the key body_text to create a corpus\n",
        "def extract_body_text(filename):\n",
        "    file = open(filename)\n",
        "    paper_content = json.load(file)\n",
        "    body_text =\"\"\n",
        "    if 'body_text' in paper_content: #look at the text that comes after the key 'body_text'\n",
        "        for bt in paper_content['body_text']:\n",
        "            body_text = body_text +bt['text']\n",
        "    return (body_text+'\\n').lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6XQGVV2a-Hb",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0645de52-273a-4353-8cc2-350b81bbeb92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#import the required libraries for preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dwQKPQ1lDIr"
      },
      "outputs": [],
      "source": [
        "#function to perform preprocessing\n",
        "def data_preprocessing(text):\n",
        "    text_low=text.lower() #converts the entire text to lower case\n",
        "    text_cit=re.sub(r\"[\\[0-9]+]\",\"\",text_low) #removes the citations\n",
        "    text_space = re.sub(' +', ' ', text_cit) #removes extra space\n",
        "    text_cit=re.sub(\"[\\x00-\\x2F\\x3A-\\x40\\x5B-\\x60\\x7B-\\x7F]+\",\" \",text_space) #remove the punctuations\n",
        "    text_num = re.sub(r'[0-9]+', '', text_cit) #removes numbers\n",
        "    text_dash = re.sub('\\-|\\+','',text_num)\n",
        "    return text_dash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-0u39Rpssmw",
        "outputId": "2c065952-1267-494b-972a-2ccd30e4f0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUtSMo8s-j1A"
      },
      "outputs": [],
      "source": [
        "#create a list of all the extracted english texts\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "corpus_list = []\n",
        "for i in files:\n",
        "    try:\n",
        "        text = extract_body_text('/content/files/pdf_json/'+ i)\n",
        "        if detect(text) == 'en': #look at only english texts\n",
        "            corpus_list.append(data_preprocessing(text)) #preprocess each file\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"NLP_Processed/processed.txt\", 'w') as file: #save the preprocessed text in a text file\n",
        "    for item in corpus_list:\n",
        "        file.write(item + '\\n')"
      ],
      "metadata": {
        "id": "74cHIbuf1AyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmnLVnEeulgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce765dad-c586-4210-b267-7e76f1707790"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54822"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#Number of text files in the corpus\n",
        "len(corpus_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qEQ6MuvH5jm"
      },
      "outputs": [],
      "source": [
        "n_files = len(corpus_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Unigram Model and the vocabulary"
      ],
      "metadata": {
        "id": "NYtP3GST7cno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKMKhkOYDOZo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "\n",
        "#Create a dictionary storing the count of each word in the corpus\n",
        "vocab = Counter()\n",
        "for text in corpus_list:\n",
        "    tokens = [token for token in text.split(\" \") if token != \"\"] #split each file into unigrams\n",
        "    for word in tokens:\n",
        "        vocab.update({word:1})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBiw2MxLDZ7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eed109e-5c73-4cbc-9c04-718f0cd934da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "821789"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Get the size of the vocabulary\n",
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHQoBCSrH5jn"
      },
      "outputs": [],
      "source": [
        "total_count = 0 #total number of words in the corpus\n",
        "for w1,count in vocab.items():\n",
        "    total_count+=count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woOADOT3nhS4"
      },
      "outputs": [],
      "source": [
        "#function to calculate the probability of occurence of each word\n",
        "def unigram_prob(word):\n",
        "  if word not in vocab.keys():  #performing laplace smoothing\n",
        "    unigram_prob = 1/(total_count+vocab_size)\n",
        "  else:\n",
        "    unigram_prob = (vocab[word]+1)/(total_count+vocab_size) #P(w1) = (count(w1)+1)/(N+|V|)\n",
        "  return unigram_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Model on all the files"
      ],
      "metadata": {
        "id": "ZR2uuED-7ijO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VQpZvVHH5jo"
      },
      "outputs": [],
      "source": [
        "bigram_model = Counter() #counter to store the bigram counts\n",
        "for i in range(n_files):\n",
        "    process_text = corpus_list[i]\n",
        "    bigrams_text = nltk.bigrams(process_text.split(\" \")) #split each file into bigrams\n",
        "    for words in bigrams_text:\n",
        "        bigram_model.update({words:1}) #increases the count for each bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL3CEM7FBntz"
      },
      "outputs": [],
      "source": [
        "bigram_first = Counter() #counter to store the list of words that appear after some word w1\n",
        "for i,j in bigram_model.items():\n",
        "    w1 = i[0]\n",
        "    w2 = i[1]\n",
        "    list1 = [w2]\n",
        "    if w1 not in bigram_first.keys():\n",
        "        bigram_first[w1] = list1\n",
        "    else:\n",
        "        bigram_first[w1].append(w2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPPm3fu7Ijt5"
      },
      "outputs": [],
      "source": [
        "#function to calculate the probability of each bigram\n",
        "def bigram_prob(w1,w2):\n",
        "  if (w1,w2) not in bigram_model.keys():\n",
        "    bigram_prob = 1/(vocab[w1]+vocab_size) #laplace smoothing\n",
        "  else:\n",
        "    bigram_prob = (bigram_model[(w1,w2)]+1)/(vocab[w1]+vocab_size) #P(w2|w1) = (count(w1,w2)+1)/(count(w1)+|V|)\n",
        "  return bigram_prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('NLP_Processed/bigram_model_file.pickle', 'wb') as f: #saving the bigram model\n",
        "    pickle.dump(bigram_model, f)"
      ],
      "metadata": {
        "id": "ceyAHM9di0vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('NLP_Processed/bigram_first_file.pickle', 'wb') as f: #saving the bigram second words\n",
        "  pickle.dump(bigram_first,f)"
      ],
      "metadata": {
        "id": "cXyOLgmJ2q2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8rDV7oDjDVj"
      },
      "outputs": [],
      "source": [
        "#Function to predict the next word of a sentence\n",
        "def predict_next_bigram(sentence):\n",
        "    word2_list = Counter()\n",
        "    sentence = data_preprocessing(sentence) #preprocess the sentence\n",
        "    sent_tokens = word_tokenize(sentence)\n",
        "    n = len(sent_tokens)\n",
        "    if bigram_first[sent_tokens[n-1]]==0: #if no word appears after the previous word then return 0\n",
        "      return 0\n",
        "    for w_n in bigram_first[sent_tokens[n-1]]: #look at words coming after w_n-1\n",
        "        word2_list.update({w_n: unigram_prob(sent_tokens[0])*bigram_prob(sent_tokens[n-1],w_n)})\n",
        "        for i in range(1,n):\n",
        "            word2_list[w_n] *= bigram_prob(sent_tokens[i-1],sent_tokens[i]) #use P(w1,w2,...w_n) = P(w1)P(w2|w1).....P(w_n|w_n-1)\n",
        "\n",
        "    return word2_list.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ExqjPfxsTx",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cf6404-e319-49f3-bf42-057c1a9bf4e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('not', 1.7251617712948942e-14),\n",
              " ('used', 1.6782872380634335e-14),\n",
              " ('also', 1.2920569907586064e-14),\n",
              " ('', 1.291112540990422e-14),\n",
              " ('performed', 1.2472204807111218e-14),\n",
              " ('found', 1.0959593941540319e-14),\n",
              " ('collected', 1.049482523982859e-14),\n",
              " ('obtained', 8.821160834841006e-15),\n",
              " ('observed', 8.195338593712591e-15),\n",
              " ('identified', 8.004460324774299e-15)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "#sentence 21\n",
        "predict_next_bigram(\"all houses were\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 2\n",
        "predict_next_bigram(\"it aims to develop an integrated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCZiW99j3f5K",
        "outputId": "77df62f7-cd7e-492f-f1f5-b6774e4a2386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('into', 4.42129173749273e-20),\n",
              " ('with', 2.3017679175427988e-20),\n",
              " ('in', 1.3156525949512529e-20),\n",
              " ('and', 1.041038201318164e-20),\n",
              " ('dna', 7.988782360235311e-21),\n",
              " ('approach', 7.339693793466193e-21),\n",
              " ('the', 5.167743589277217e-21),\n",
              " ('to', 4.668444691762509e-21),\n",
              " ('care', 4.543619967383833e-21),\n",
              " ('moving', 3.619917006981625e-21)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible1 = []\n",
        "for i in predict_next_bigram(\"it aims to develop an integrated\"): #based on the gap the words that fill up the second gap\n",
        "  possible1.append(\"it aims to develop an integrated \"+i[0]+' to reach mmps exposed to malaria with prevention diagnosis and treatment')\n",
        "predicted1 = []\n",
        "for i in possible1:\n",
        "  for j in predict_next_bigram(i):\n",
        "    predicted1.append(j)"
      ],
      "metadata": {
        "id": "HKnGRNSc3vv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted1 = sorted(predicted1, key = lambda x: x[1])\n",
        "top_10_1 = sorted1[-10:]\n",
        "top_10_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o8fAaSe-iMg",
        "outputId": "85061c8d-5cb9-4fd1-ce27-8b84f6272e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('for', 9.947503619600165e-62),\n",
              " ('is', 1.0665808851660208e-61),\n",
              " ('and', 1.2079111538085919e-61),\n",
              " ('in', 1.3935217409130527e-61),\n",
              " ('with', 1.7345056441416422e-61),\n",
              " ('for', 1.87934974150911e-61),\n",
              " ('and', 2.282067543261063e-61),\n",
              " ('with', 3.276945511777266e-61),\n",
              " ('of', 4.302893718306996e-61),\n",
              " ('of', 8.129318175172942e-61)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible2 = []\n",
        "for i in possible1:\n",
        "  for j in top_10_1:\n",
        "    possible2.append(i+' '+j[0]+\" by involving non-health \")\n",
        "predicted2 = []\n",
        " #based on the previous gaps the words that fill up the third gap\n",
        "for i in possible2:\n",
        "  for j in predict_next_bigram(i):\n",
        "    predicted2.append(j)\n",
        "sorted2 = sorted(list(set(predicted2)), key = lambda x: x[1])\n",
        "top_10_2 = sorted2[-10:]\n",
        "top_10_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZxQefuV4UxS",
        "outputId": "6c4fdd7c-6a4b-414e-a07f-b8c45368a6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('services', 8.654908489557309e-79),\n",
              " ('organization', 9.464091456963222e-79),\n",
              " ('system', 9.958182355239804e-79),\n",
              " ('care', 1.0670493948463225e-78),\n",
              " ('care', 1.228543850522251e-78),\n",
              " ('and', 1.6819986225658078e-78),\n",
              " ('organization', 1.788020242407041e-78),\n",
              " ('care', 2.7551060025264833e-78),\n",
              " ('and', 3.1777456911996464e-78),\n",
              " ('care', 5.205132816917198e-78)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_bigram(\"this is because engineers do not work in\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FsMEAfa7uRW",
        "outputId": "34815ee3-850b-4d4e-8ffb-eb14be17909e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 1.8222372751045392e-25),\n",
              " ('a', 3.4239926456058674e-26),\n",
              " ('', 3.0937223390173425e-26),\n",
              " ('this', 2.4620413318692996e-26),\n",
              " ('addition', 1.3828992948122097e-26),\n",
              " ('patients', 9.339450012332356e-27),\n",
              " ('our', 8.601503486469278e-27),\n",
              " ('order', 8.03802501183751e-27),\n",
              " ('which', 7.634386699425513e-27),\n",
              " ('vitro', 6.834119271419552e-27)]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#funtion to calculate perplexity of a sentence with bigram model\n",
        "def perplexity_bigram(sentence):\n",
        "    process_text = data_preprocessing(sentence)\n",
        "    sent_tokens = word_tokenize(process_text)\n",
        "    n = len(sent_tokens)\n",
        "    prod = unigram_prob(sent_tokens[0]) #calculating the product of probablities\n",
        "    for i in range(1,n):\n",
        "        prod *= bigram_prob(sent_tokens[i-1],sent_tokens[i])  #calculating the probability\n",
        "    perplexity_bigram = (1/prod)**(1/n)  #calculating the perplexity\n",
        "    return perplexity_bigram"
      ],
      "metadata": {
        "id": "FN52g3eJkf21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 1\n",
        "perplexity_bigram('it appears that the overall code stroke volume has decreased since the covid- pandemic.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU46F1XdkrRl",
        "outputId": "a687e44a-6840-4651-d4e8-e82083402647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115.5835591162884"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 2\n",
        "perplexity_bigram('half a century ago hypertension was not treatable.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utu29vE1ktu9",
        "outputId": "d352aec9-80db-4134-9e12-a619822b7395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5077.3920193259155"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 3\n",
        "perplexity_bigram('sarahs tv is broadcasting an advert for private healthcare.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ_0LKPLkm6-",
        "outputId": "76ce6975-75ca-4387-9447-a148c7e23d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215654.68380117096"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram Model on 10000 files"
      ],
      "metadata": {
        "id": "qEfeK2ra7-Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_files = 10000"
      ],
      "metadata": {
        "id": "PawNPr1YlAPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nosMYbKD113"
      },
      "outputs": [],
      "source": [
        "#counter to store the count of each trigram\n",
        "trigram_model = Counter()\n",
        "for i in range(n_files):\n",
        "    process_text = corpus_list[i]\n",
        "    trigrams_text = nltk.trigrams(process_text.split(\" \"))\n",
        "    for words in trigrams_text:\n",
        "        trigram_model.update({words:1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_first = Counter() #dictionary storing the count of w3 if (w1,w2) occured\n",
        "for i,j in trigram_model.items():\n",
        "    w1 = i[0]\n",
        "    w2 = i[1]\n",
        "    w3 = i[2]\n",
        "    list1 = [w3]\n",
        "    if (w1,w2) not in trigram_first.keys():\n",
        "        trigram_first[(w1,w2)] = list1 #create a dictionary for each (w1,w2)\n",
        "    else:\n",
        "      trigram_first[(w1,w2)].append(w3)"
      ],
      "metadata": {
        "id": "oORvo39zmz5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function calculating the probability of a trigram\n",
        "#uses the equation: P(w1,w2,....wn) = P(w1)P(w2|w1)P(w3|w1,w2)...P(wn|w_n-2,w_n-1)\n",
        "def trigram_prob(w1,w2,w3):\n",
        "  if (w1,w2,w3) not in trigram_model.keys():\n",
        "    trigram_prob = 1/(bigram_model[(w1,w2)]+vocab_size)\n",
        "  else:\n",
        "    trigram_prob = (trigram_model[(w1,w2,w3)]+1)/(bigram_model[(w1,w2)]+vocab_size)\n",
        "  return trigram_prob"
      ],
      "metadata": {
        "id": "zqFfBrfStsxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dav1dXZxeAwr"
      },
      "outputs": [],
      "source": [
        "#Function to predict the next word of a sentence\n",
        "def predict_next_trigram(sentence):\n",
        "  word3_list = Counter()\n",
        "  sentence = data_preprocessing(sentence) #preprocess the text\n",
        "  sent_tokens = word_tokenize(sentence)\n",
        "  n = len(sent_tokens)\n",
        "  if trigram_first[(sent_tokens[n-2],sent_tokens[n-1])] == 0:  #if no word appears after the previous word then return 0\n",
        "    return 0\n",
        "  for w_n in trigram_first[(sent_tokens[n-2],sent_tokens[n-2])]:\n",
        "      word3_list.update({w_n: unigram_prob(sent_tokens[0])*bigram_prob(sent_tokens[0],sent_tokens[1])*trigram_prob(sent_tokens[n-2],sent_tokens[n-1],w_n)})\n",
        "      for i in range(2,n):\n",
        "          word3_list[w_n] *= trigram_prob(sent_tokens[i-2],sent_tokens[i-1],sent_tokens[i])\n",
        "  return word3_list.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcIXzauOH5js"
      },
      "outputs": [],
      "source": [
        "#sentence 1\n",
        "predict_next_trigram(\"all houses were\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 2\n",
        "predict_next_trigram(\"it aims to develop an integrated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBzuUu1qyU5T",
        "outputId": "52c752ac-a239-4426-faf8-fa276f71556d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 9.503699058615132e-30),\n",
              " ('mrna', 7.91974921551261e-31),\n",
              " ('angel', 7.91974921551261e-31),\n",
              " ('', 7.91974921551261e-31),\n",
              " ('would', 7.91974921551261e-31),\n",
              " ('an', 7.91974921551261e-31),\n",
              " ('interactive', 7.91974921551261e-31),\n",
              " ('essential', 7.91974921551261e-31),\n",
              " ('erent', 7.91974921551261e-31),\n",
              " ('m', 7.91974921551261e-31)]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible1 = []\n",
        "for i in predict_next_trigram(\"it aims to develop an integrated\"): #based on the gap the words that fill up the second gap\n",
        "  possible1.append(\"it aims to develop an integrated \"+i[0]+' to reach mmps exposed to malaria with prevention diagnosis and treatment')\n",
        "predicted1 = []\n",
        "for i in possible1:\n",
        "  for j in predict_next_trigram(i):\n",
        "    predicted1.append(j)\n",
        "\n",
        "sorted1 = sorted(predicted1, key = lambda x: x[1])\n",
        "top_10_1 = sorted1[-10:]\n",
        "top_10_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9vnfAXw83t6",
        "outputId": "528ece54-66df-4dec-d5b7-a63a30656de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('on', 2.2612794439702113e-88),\n",
              " ('these', 2.2612794439702113e-88),\n",
              " ('a', 4.070302999146378e-88),\n",
              " ('will', 4.9748147767344645e-88),\n",
              " ('to', 7.688350109498716e-88),\n",
              " ('was', 9.045117775880845e-88),\n",
              " ('are', 1.1306397219851053e-87),\n",
              " ('the', 1.6281211996585512e-87),\n",
              " ('', 1.6281211996585512e-87),\n",
              " ('in', 2.3969562106084226e-87)]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_trigram(\"to reach mmps exposed to malaria with prevention diagnosis and treatment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvOMYyaxaGc",
        "outputId": "214eb8d4-74a1-4560-c30f-dd397d765dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('of', 8.64399824510451e-55),\n",
              " ('with', 1.0169409700122955e-55),\n",
              " ('for', 6.944962722035188e-56),\n",
              " ('in', 6.572911147640445e-56),\n",
              " ('is', 4.836670467131649e-56),\n",
              " ('the', 4.464618892736906e-56),\n",
              " ('', 4.464618892736906e-56),\n",
              " ('are', 3.100429786622853e-56),\n",
              " ('services', 2.72837821222811e-56),\n",
              " ('was', 2.480343829298282e-56)]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible2 = []\n",
        "for i in possible1:\n",
        "  for j in top_10_1:\n",
        "    possible2.append(i+' '+j[0]+\" by involving non-health \")\n",
        "predicted2 = []\n",
        " #based on the previous gaps the words that fill up the third gap\n",
        "for i in possible2:\n",
        "  for j in predict_next_trigram(i):\n",
        "    predicted2.append(j)\n",
        "sorted2 = sorted(list(set(predicted2)), key = lambda x: x[1])\n",
        "top_10_2 = sorted2[-10:]\n",
        "top_10_2"
      ],
      "metadata": {
        "id": "sXmOZp0Q9FyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_trigram(\"this is because engineers do not work in\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTrlwaIB9Z7T",
        "outputId": "7ca8bd56-a909-48ab-af03-5379cab4fb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 9.993780333687907e-40), ('associated', 4.9968901668439536e-40)]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGSPv-IRbvPJ"
      },
      "outputs": [],
      "source": [
        "#funtion to calculate perplexity of a sentence with bigram model\n",
        "def perplexity_trigram(sentence):\n",
        "    process_text = data_preprocessing(sentence)\n",
        "    sent_tokens = word_tokenize(sentence)\n",
        "    n = len(sent_tokens)\n",
        "    prod = unigram_prob(sent_tokens[0])*bigram_prob(sent_tokens[0],sent_tokens[1])\n",
        "    for i in range(2,n):\n",
        "        prod *= trigram_prob(sent_tokens[i-2], sent_tokens[i-1],sent_tokens[i]) #calculatiung the probability\n",
        "    perplexity_trigram = (1/prod)**(1/n)\n",
        "    return perplexity_trigram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 1\n",
        "perplexity_trigram(\"it appears that the overall code stroke volume has decreased since the covid- pandemic.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORTyo5bDzHtv",
        "outputId": "048b5aa1-9a1a-4224-d135-51b5e6dca62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83959.6184190862"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 3\n",
        "perplexity_trigram('half a century ago hypertension was not treatable.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHuWV2UXzOxK",
        "outputId": "530d5e82-c717-4f3a-d50c-3e8dc971cbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87466.6507931325"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence 3\n",
        "perplexity_trigram('sarahs tv is broadcasting an advert for private healthcare.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAvJiWSizOtw",
        "outputId": "43ca8cfa-4cb0-4d96-f4f9-0f45b2680d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1430193.9754503905"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBzYb7prH5jt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "892740d3-7e5e-44c5-dff7-9cd133b1cd1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-93-62917a568404>\", line 1, in <module>\n",
            "    with open('NLP_Processed/trigram_model_file.pickle', 'wt') as f:\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'NLP_Processed/trigram_model_file.pickle'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 737, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 721, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.8/posixpath.py\", line 379, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "with open('NLP_Processed/trigram_model_file.pickle', 'wt') as f: #saving the trigram_model\n",
        "  pickle.dump(trigram_model,f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the trigram third word corresponding to first two\n",
        "with open('NLP_Processed/trigram_first_file.pickle', 'wt') as f:\n",
        "  pickle.dump(trigram_first,f)"
      ],
      "metadata": {
        "id": "Ol4UITxCIE-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "60aec64e-5ceb-4927-d94a-71a4890a1c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-94-4a4b15e2beaf>\", line 1, in <module>\n",
            "    with open('NLP_Processed/trigram_first_file.pickle', 'wt') as f:\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'NLP_Processed/trigram_first_file.pickle'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 737, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 721, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.8/posixpath.py\", line 379, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5k38TEQs3CSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}